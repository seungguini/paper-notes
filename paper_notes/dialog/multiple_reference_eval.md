# Investigating Evaluation of Open-Domain Dialogue Systems With Human Generated Multiple References

This paper introduces demonstrates the effectiveness of multi-reference evaluation of open-domain dialogue systems when compared to human judgements

## Key Points

### Intro

- Many dialogue prompts have single references for a valid resposne.
- This penalizes various responses that could be a contextually valid response to the given promts. This is called a _one-to-many_ problem
- Autmoatic evaluation may also disproportionately favor models that produce generic responses ("I don't know")

### Unreferenced Diversity

- Prior work focuses on unreferenced diversity, which computes the lexical diversity of responses generated by the model. It does NOT check
  the model's responses with the references.
- lower Self-BLEU indicates higher diversity

### Multireference Evaluation & Single Output

- With a set of all plausible references, the metric score equals the maximum of the scores dervied from the response paired with each reference

### Multireference Evaluation & Multiple Outputs

- For each reference, the score equals the highest score for each response. Then, the highest scores for each reference is averaged for the final score.

### Metrics

1. Word-overlap metrics
2. Embedding-based metrics

### Models Evaluated

1. **Human** - the _ideal_ model for a given context
2. **Dual Encoder** - LTSM based
3. **Seq2Seq** - LTSM encoder, LTSM decoder, and attention mechanism
4. **HRED**- Hierarchial Recurrent Encoder Decoder
5. **CVAE** -

## Code

- `test_duid_mapping.json` - a Python `dict` of dialogue - context id pairs.

  - _EXAMPLE_ : `"970_4" : 6552` --> **Dialogue # 970**, **utterance # 4** belongs to **context # 6552**
  - The utterances inside **1000** dialogues maps to a total of **6740 contexts**

- `hredf.txt` - 6740 lines long model output for the HREDF model. Each line is the model-generated output (next utterance) in response to the context in multireftest.json
- `multireftest.json` - 1000 lines of multiple references per dialogue.
- `test.tgt` - 6740 lines of single references. The single reference is simply the first of the multiple references in `multireftest.json`. Refer to `test_duid_mapping.json` for specific dialogue_utterance combo.

### Random Notes

**Retrieval-based dialogue systems** - conversation systems that _retrieve_ responses based on a set of candidates
**Generation-based dialogue systems** - conversation systems that _generate_ response
_[reference](https://arxiv.org/pdf/1907.12878.pdf)_

## References

- [Investigating Evaluation of Open-Domain Dialogue Systems With Human Generated Multiple References](https://arxiv.org/pdf/1705.06476.pdf) by Gupta et al.
