# Attenion is All You Need
[Paper](https://arxiv.org/pdf/1706.03762.pdf) from Google

## Transformers
- avoids using recurrence - instead relies on an attention mechanism